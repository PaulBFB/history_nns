\chapter{Revival and Recent Explosion}

In 2007 \cite{hinton2009deep} published his paper on "Deep Belief Networks"; a layered composite model that was used in image recognition. 
However, the approach still suffered from several limitations. Firstly, using Hinton's method of backpropagation was promising, but the deep belief nets at the time still suffered from what is known as "vanishing gradients". The error that was backpropagated through the network to adjust all parameters grew vanishingly small after propagation through multiple layers - due to the fact that backpropagation involved multiple multiplications by numbers between 0 and 1.

This was due to the fact that backpropagation builds a partial derivative of the loss function with regard to each weight, which was in relation to the slope of the activation function. The most popular activation function at the time, the \textit{sigmoid} activation function, has a gradient approaching 0 at large and small values. This leads to vanishing gradients, or saturated activation functions.

At the time it was standard procedure, therefore, to pre-train lower layers of the network separately, in order adjust their weights.

In 2010 \cite{glorot2010understanding} proposed a new initialization function for weights, a normalized random activation, henceforth the de-facto standard known as glorot-initialization (which is still used by default in packages like keras). This initialization, with the advent of new activation functions such as the hyperbolic tangent activation function, significantly ameliorated the problem of saturation.

Finally in 2012, two more important pieces emerged to kick off the recent explosion off deep learning, which is still ongoing. 

\textit{Dropout} which randomly deactivates a number of neurons of a neural network layer during training in order to make sure training signals "saturate" each region and that neurons do not overfit each other's signals too closely as described by \cite{dropout} (thereby making neural networks much more robust).

Secondly, \textit{RMSPropagation} as described by \cite{RMSPROP}, in which batches are split into smaller mini batches and adjust the change in weights based on the root mean square (hence RMS) of the batch, thereby enabling smaller batches of very large datasets being used. 

As soon as these pieces were in place, a veritable cambrian explosion of deep learning improvements and technologies followed soon after, the most important of which are:

\begin{itemize}
	\item BatchNormalization
	\item Wide \& Deep learning Recommender Systems
	\item Generative Adversarial Networks 
	\item Monte-Carlo Dropout
	\item Word Embeddings for natural language understanding
	\item Recurrent Neural Networks for Sequence and timeseries processing
	\item Convolutional Neural Networks for computer vision
\end{itemize}

The advances of 2012 have kicked off a veritable tsunami of research and development of neural networks and the field itself is still very much in flux. Arguably, only a very small percentage of these advances have, in turn, as of yet found their way into application in business and public sectors at all.

As for the ever-elusive promise of \textbf{TRUE} artificial intelligence, or artificial general intelligence as it is now referred to - while some researchers and public intellectuals such as Sam Harris have been very vocals about the risk of the impending "singularity" and its' attendant risks, others like \cite{thousand} in his book "a thousand brains" argue that while \ac{AI} is coming, it is not cause for alarm while Geoff Hinton himself apparently stated that deep networks alone may not be enough for true \ac{AI} and we need to look towards other ways than backpropagation.

It may therefore be taken for granted that the field itself will remain of interest for years to come, even though another \ac{AI} winter is not out of the realm of possibility.
