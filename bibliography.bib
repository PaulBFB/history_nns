@misc{golgi,
	author = {Golgi, Camillo and Cajal, Santiago Ramon},
title = {On the Structure of the Nervous System},
year = {1906},
url = {https://www.nobelprize.org/prizes/medicine/1906/summary/},
}

@article{mcculloch1943logical,
	title={A logical calculus of the ideas immanent in nervous activity},
	author={McCulloch, Warren S and Pitts, Walter},
	journal={The bulletin of mathematical biophysics},
	volume={5},
	number={4},
	pages={115--133},
	year={1943},
	publisher={Springer}
}

@ARTICLE{babbage,
	author={Bromley, Allan G.},
	journal={Annals of the History of Computing}, 
	title={Charles Babbage's Analytical Engine, 1838}, 
	year={1982},
	volume={4},
	number={3},
	pages={196-217},
	doi={10.1109/MAHC.1982.10028}}

@article{krizhevsky2012imagenet,
	title={Imagenet classification with deep convolutional neural networks},
	author={Krizhevsky, Alex and Sutskever, Ilya and Hinton, Geoffrey E},
	journal={Advances in neural information processing systems},
	volume={25},
	pages={1097--1105},
	year={2012}
}

@article{turing1950mind,
	title={Mind},
	author={Turing, Alan Mathison},
	journal={Mind},
	volume={59},
	number={236},
	pages={433--460},
	year={1950}
}

@article{minsky,
	title={Marvin Minsky and Seymour Papert, Perceptrons, An Introduction to Computational Geometry},
	author={Mycielski, Jan},
	journal={Bulletin of the American Mathematical Society},
	volume={78},
	number={1},
	pages={12--15},
	year={1972},
	publisher={American Mathematical Society}
}

@book{hebb1949organisation,
	title={The organisation of behaviour: a neuropsychological theory},
	author={Hebb, Donald Olding},
	year={1949},
	publisher={Science Editions New York}
}

@article{rosenblatt1958perceptron,
	title={The perceptron: a probabilistic model for information storage and organization in the brain.},
	author={Rosenblatt, Frank},
	journal={Psychological review},
	volume={65},
	number={6},
	pages={386},
	year={1958},
	publisher={American Psychological Association}
}

@article{backprop_werbos,
	author = {Werbos, Paul and John, Paul},
	year = {1974},
	month = {01},
	pages = {},
	title = {Beyond regression : new tools for prediction and analysis in the behavioral sciences /}
}

@techreport{backprop,
	title={Learning internal representations by error propagation},
	author={Rumelhart, David E and Hinton, Geoffrey E and Williams, Ronald J},
	year={1985},
	institution={California Univ San Diego La Jolla Inst for Cognitive Science}
}

@article{hinton2009deep,
	title={Deep belief networks},
	author={Hinton, Geoffrey E},
	journal={Scholarpedia},
	volume={4},
	number={5},
	pages={5947},
	year={2009}
}

@inproceedings{glorot2010understanding,
	title={Understanding the difficulty of training deep feedforward neural networks},
	author={Glorot, Xavier and Bengio, Yoshua},
	booktitle={Proceedings of the thirteenth international conference on artificial intelligence and statistics},
	pages={249--256},
	year={2010},
	organization={JMLR Workshop and Conference Proceedings}
}

@misc{RMSPROP,
	author = {Hinton, Geoffrey E},
	title = {Batch Gradient Descent},
	year = {2012},
	url = {https://www.cs.toronto.edu/~tijmen/csc321/slides/lecture_slides_lec6.pdf},
}

@misc{dropout,
	title={Improving neural networks by preventing co-adaptation of feature detectors}, 
	author={Geoffrey E. Hinton and Nitish Srivastava and Alex Krizhevsky and Ilya Sutskever and Ruslan R. Salakhutdinov},
	year={2012},
	eprint={1207.0580},
	archivePrefix={arXiv},
	primaryClass={cs.NE}
}

@book{thousand,
	title={A Thousand Brains: A new Theory of Intelligence}, 
	author={Hawkins, Jeff},
	year={2021}


}

@article{mice,
	author = {Buuren, Stef and Groothuis-Oudshoorn, Catharina},
	year = {2011},
	month = {12},
	pages = {},
	title = {MICE: Multivariate Imputation by Chained Equations in R},
	volume = {45},
	journal = {Journal of Statistical Software},
	doi = {10.18637/jss.v045.i03}
}

@article {norvig_eod,
	author = {F. Pereira and P. Norvig and A. Halevy},
	journal = {IEEE Intelligent Systems},
	title = {The Unreasonable Effectiveness of Data},
	year = {2009},
	volume = {24},
	number = {02},
	issn = {1941-1294},
	pages = {8-12},
	keywords = {machine learning;very large data bases;semantic web},
	doi = {10.1109/MIS.2009.36},
	publisher = {IEEE Computer Society},
	address = {Los Alamitos, CA, USA},
	month = {mar}
}

@misc{buitinck2013api,
	title={API design for machine learning software: experiences from the scikit-learn project}, 
	author={Lars Buitinck and Gilles Louppe and Mathieu Blondel and Fabian Pedregosa and Andreas Mueller and Olivier Grisel and Vlad Niculae and Peter Prettenhofer and Alexandre Gramfort and Jaques Grobler and Robert Layton and Jake Vanderplas and Arnaud Joly and Brian Holt and Gaël Varoquaux},
	year={2013},
	eprint={1309.0238},
	archivePrefix={arXiv},
	primaryClass={cs.LG}
}


@misc{smith_1cycle,
	title={A disciplined approach to neural network hyper-parameters: Part 1 -- learning rate, batch size, momentum, and weight decay}, 
	author={Leslie N. Smith},
	year={2018},
	eprint={1803.09820},
	archivePrefix={arXiv},
	primaryClass={cs.LG}
}

@misc{goodfellow2014generative,
	title={Generative Adversarial Networks}, 
	author={Ian J. Goodfellow and Jean Pouget-Abadie and Mehdi Mirza and Bing Xu and David Warde-Farley and Sherjil Ozair and Aaron Courville and Yoshua Bengio},
	year={2014},
	eprint={1406.2661},
	archivePrefix={arXiv},
	primaryClass={stat.ML}
}

@article{mnist,
	title={The mnist database of handwritten digit images for machine learning research},
	author={Deng, Li},
	journal={IEEE Signal Processing Magazine},
	volume={29},
	number={6},
	pages={141--142},
	year={2012},
	publisher={IEEE}
}

@inproceedings{titanic,
	author = {Farag, Nadine and Hassan, Ghada},
	title = {Predicting the Survivors of the Titanic Kaggle, Machine Learning From Disaster},
	year = {2018},
	isbn = {9781450364690},
	publisher = {Association for Computing Machinery},
	address = {New York, NY, USA},
	url = {https://doi.org/10.1145/3220267.3220282},
	doi = {10.1145/3220267.3220282},
	abstract = {April 14th, 1912 was very unfortunate for the most powerful ship ever built at that
	time, the Titanic. Grievously, 1503 out of 2203 passengers perished the sinking, but
	the rationale behind survival still remains a question mark. In efforts to study the
	Titanic passengers; Kaggle, a popular data science website, assembled information
	about each passenger back in the days of the Titanic into a dataset, and made it available
	for a competition titled: "Titanic: Machine Learning from Disaster." This research
	aims to use machine learning techniques on the Titanic data to analyze the data for
	classification and to predict the survival of the Titanic passengers by using data-mining
	algorithms; specifically Decision Trees and Na\"{\i}ve Bayes. The prediction and efficiency
	of these algorithms depend greatly on data analysis and the model. The paper presents
	an implementation which combines the benefits of feature selection and machine learning
	to accurately select and distinguish characteristics of passengers' age, class, cabin,
	and port of embarkation then consequently infer an authentic model for an accurate
	prediction. The data-set is described and the implementation details and prediction
	results are presented then compared to other results. The Decision Tree algorithm
	has accurately predicted 90.01% of the survival of passengers, while the Gaussian
	Na\"{\i}ve Bayes witnessed 92.52% accuracy in prediction.},
	booktitle = {Proceedings of the 7th International Conference on Software and Information Engineering},
	pages = {32–37},
	numpages = {6},
	keywords = {Na\"{\i}ve Bayes, Machine Learning, Decision Trees, Supervised Learning, Kaggle, Data Mining},
	location = {Cairo, Egypt},
	series = {ICSIE '18}
}

@misc{gan_continual_learning,
	title={Generative Adversarial Network Training is a Continual Learning Problem}, 
	author={Kevin J Liang and Chunyuan Li and Guoyin Wang and Lawrence Carin},
	year={2018},
	eprint={1811.11083},
	archivePrefix={arXiv},
	primaryClass={stat.ML}
}

@incollection{catastrophic_forgetting,
	title={Catastrophic interference in connectionist networks: The sequential learning problem},
	author={McCloskey, Michael and Cohen, Neal J},
	booktitle={Psychology of learning and motivation},
	volume={24},
	pages={109--165},
	year={1989},
	publisher={Elsevier}
}

@misc{mode_collapse,
	title={Mode Regularized Generative Adversarial Networks}, 
	author={Tong Che and Yanran Li and Athul Paul Jacob and Yoshua Bengio and Wenjie Li},
	year={2017},
	eprint={1612.02136},
	archivePrefix={arXiv},
	primaryClass={cs.LG}
}

@misc{dcgan,
	title={Unsupervised Representation Learning with Deep Convolutional Generative Adversarial Networks}, 
	author={Alec Radford and Luke Metz and Soumith Chintala},
	year={2016},
	eprint={1511.06434},
	archivePrefix={arXiv},
	primaryClass={cs.LG}
}

@article{oversampling_gan,
	author = {Suh, Sungho and Lee, Haebom and Jo, Jun and Lukowicz, Paul and Lee, Yong},
	year = {2019},
	month = {02},
	pages = {746},
	title = {Generative Oversampling Method for Imbalanced Data on Bearing Fault Detection and Diagnosis},
	volume = {9},
	journal = {Applied Sciences},
	doi = {10.3390/app9040746}
}

@misc{batchnorm,
	title={Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift}, 
	author={Sergey Ioffe and Christian Szegedy},
	year={2015},
	eprint={1502.03167},
	archivePrefix={arXiv},
	primaryClass={cs.LG}
}

@misc{convolution_arithmetic,
	title={A guide to convolution arithmetic for deep learning}, 
	author={Vincent Dumoulin and Francesco Visin},
	year={2018},
	eprint={1603.07285},
	archivePrefix={arXiv},
	primaryClass={stat.ML}
}

@misc{arjovsky2017wasserstein,
	title={Wasserstein GAN}, 
	author={Martin Arjovsky and Soumith Chintala and Léon Bottou},
	year={2017},
	eprint={1701.07875},
	archivePrefix={arXiv},
	primaryClass={stat.ML}
}

@misc{gulrajani2017improved,
	title={Improved Training of Wasserstein GANs}, 
	author={Ishaan Gulrajani and Faruk Ahmed and Martin Arjovsky and Vincent Dumoulin and Aaron Courville},
	year={2017},
	eprint={1704.00028},
	archivePrefix={arXiv},
	primaryClass={cs.LG}
}

@article{fan2008liblinear,
	title={LIBLINEAR: A library for large linear classification},
	author={Fan, Rong-En and Chang, Kai-Wei and Hsieh, Cho-Jui and Wang, Xiang-Rui and Lin, Chih-Jen},
	journal={the Journal of machine Learning research},
	volume={9},
	pages={1871--1874},
	year={2008},
	publisher={JMLR. org}
}

@article{vapnik1995support,
	title={Support vector machines},
	author={Vapnik, Vladimir and Guyon, Isabel and Hastie, Trevor},
	journal={Mach. Learn},
	volume={20},
	number={3},
	pages={273--297},
	year={1995}
}

@article{quinlan1986induction,
	title={Induction of decision trees},
	author={Quinlan, J. Ross},
	journal={Machine learning},
	volume={1},
	number={1},
	pages={81--106},
	year={1986},
	publisher={Springer}
}

@article{winequality,
	title = {Modeling wine preferences by data mining from physicochemical properties},
	journal = {Decision Support Systems},
	volume = {47},
	number = {4},
	pages = {547-553},
	year = {2009},
	note = {Smart Business Networks: Concepts and Empirical Evidence},
	issn = {0167-9236},
	doi = {https://doi.org/10.1016/j.dss.2009.05.016},
	url = {https://www.sciencedirect.com/science/article/pii/S0167923609001377},
	author = {Paulo Cortez and António Cerdeira and Fernando Almeida and Telmo Matos and José Reis},
	keywords = {Sensory preferences, Regression, Variable selection, Model selection, Support vector machines, Neural networks},
	abstract = {We propose a data mining approach to predict human wine taste preferences that is based on easily available analytical tests at the certification step. A large dataset (when compared to other studies in this domain) is considered, with white and red vinho verde samples (from Portugal). Three regression techniques were applied, under a computationally efficient procedure that performs simultaneous variable and model selection. The support vector machine achieved promising results, outperforming the multiple regression and neural network methods. Such model is useful to support the oenologist wine tasting evaluations and improve wine production. Furthermore, similar techniques can help in target marketing by modeling consumer tastes from niche markets.}
}

@inproceedings{diabetes,
	title={Using the ADAP learning algorithm to forecast the onset of diabetes mellitus},
	author={Smith, Jack W and Everhart, James E and Dickson, WC and Knowler, William C and Johannes, Robert Scott},
	booktitle={Proceedings of the annual symposium on computer application in medical care},
	pages={261},
	year={1988},
	organization={American Medical Informatics Association}
}

@MISC{bagging,
	author = {Leo Breiman},
	title = {Bagging Predictors},
	year = {1994}
}

@article{conditional_gans,
	author    = {Mehdi Mirza and
	Simon Osindero},
	title     = {Conditional Generative Adversarial Nets},
	journal   = {CoRR},
	volume    = {abs/1411.1784},
	year      = {2014},
	url       = {http://arxiv.org/abs/1411.1784},
	eprinttype = {arXiv},
	eprint    = {1411.1784},
	timestamp = {Mon, 13 Aug 2018 16:48:15 +0200},
	biburl    = {https://dblp.org/rec/journals/corr/MirzaO14.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org}
}

@article{smote,
	title={SMOTE: Synthetic Minority Over-sampling Technique},
	volume={16},
	ISSN={1076-9757},
	url={http://dx.doi.org/10.1613/jair.953},
	DOI={10.1613/jair.953},
	journal={Journal of Artificial Intelligence Research},
	publisher={AI Access Foundation},
	author={Chawla, N. V. and Bowyer, K. W. and Hall, L. O. and Kegelmeyer, W. P.},
	year={2002},
	month={Jun},
	pages={321–357}
}

@incollection{mathematics_unreasonable,
	title={The unreasonable effectiveness of mathematics in the natural sciences},
	author={Wigner, Eugene P},
	booktitle={Mathematics and Science},
	pages={291--306},
	year={1990},
	publisher={World Scientific}
}

@book{dataprep,
	title={Data preparation for machine learning: data cleaning, feature selection, and data transforms in Python},
	author={Brownlee, Jason},
	year={2020},
	publisher={Machine Learning Mastery}
}

@article{datawrangling_time,
	author = {Munson, M. Arthur},
	title = {A Study on the Importance of and Time Spent on Different Modeling Steps},
	year = {2012},
	issue_date = {December 2011},
	publisher = {Association for Computing Machinery},
	address = {New York, NY, USA},
	volume = {13},
	number = {2},
	issn = {1931-0145},
	url = {https://doi.org/10.1145/2207243.2207253},
	doi = {10.1145/2207243.2207253},
	abstract = {Applying data mining and machine learning algorithms requires many steps to prepare
	data and to make use of modeling results. This study investigates two questions: (1)
	how time consuming are the pre- and post-processing steps? (2) how much research energy
	is spent on these steps? To answer these questions I surveyed practitioners about
	their experiences in applying modeling techniques and categorized data mining and
	machine learning research papers from 2009 according to the modeling step(s) they
	addressed. Survey results show that model building consumes only 14% of the time spent
	on a typical project; the remaining time is spent on pre- and post-processing steps.
	Both survey responses and the categorization of research papers show that data mining
	and machine learning researchers spend the majority of their energy on algorithms
	for constructing models and significantly less energy on other steps. These findings
	collectively suggest that there are research opportunities to simplify the steps that
	precede and follow model building.},
	journal = {SIGKDD Explor. Newsl.},
	month = may,
	pages = {65–71},
	numpages = {7}
}

@article{raudys1991small,
	title={Small sample size effects in statistical pattern recognition: Recommendations for practitioners},
	author={Raudys, Sarunas J and Jain, Anil K and others},
	journal={IEEE Transactions on pattern analysis and machine intelligence},
	volume={13},
	number={3},
	pages={252--264},
	year={1991}
}

@book{swingler1996applying,
	title={Applying neural networks: a practical guide},
	author={Swingler, Kevin},
	year={1996},
	publisher={Morgan Kaufmann}
}

@book{chollet2017deep,
	title={Deep learning with Python},
	author={Chollet, Francois},
	year={2017},
	publisher={Simon and Schuster}
}

@book{geron2019hands,
	title={Hands-on machine learning with Scikit-Learn, Keras, and TensorFlow: Concepts, tools, and techniques to build intelligent systems},
	author={G{\'e}ron, Aur{\'e}lien},
	year={2019},
	publisher={O'Reilly Media}
}

@misc{wiatrak2020stabilizing,
	title={Stabilizing Generative Adversarial Networks: A Survey}, 
	author={Maciej Wiatrak and Stefano V. Albrecht and Andrew Nystrom},
	year={2020},
	eprint={1910.00927},
	archivePrefix={arXiv},
	primaryClass={cs.LG}
}

@book{el2020practical,
	title={Practical Synthetic Data Generation: Balancing Privacy and the Broad Availability of Data},
	author={El Emam, Khaled and Mosquera, Lucy and Hoptroff, Richard},
	year={2020},
	publisher={O'Reilly Media}
}

@article{srivastava2014dropout,
	title={Dropout: a simple way to prevent neural networks from overfitting},
	author={Srivastava, Nitish and Hinton, Geoffrey and Krizhevsky, Alex and Sutskever, Ilya and Salakhutdinov, Ruslan},
	journal={The journal of machine learning research},
	volume={15},
	number={1},
	pages={1929--1958},
	year={2014},
	publisher={JMLR. org}
}

@inproceedings{dietterich2000ensemble,
	title={Ensemble methods in machine learning},
	author={Dietterich, Thomas G},
	booktitle={International workshop on multiple classifier systems},
	pages={1--15},
	year={2000},
	organization={Springer}
}

@inproceedings{kfold,
	title={K-Fold Cross Validation for Error Rate Estimate in Support Vector Machines.},
	author={Anguita, Davide and Ghio, Alessandro and Ridella, Sandro and Sterpi, Dario},
	booktitle={DMIN},
	pages={291--297},
	year={2009}
}

@inproceedings{patki2016synthetic,
	title={The synthetic data vault},
	author={Patki, Neha and Wedge, Roy and Veeramachaneni, Kalyan},
	booktitle={2016 IEEE International Conference on Data Science and Advanced Analytics (DSAA)},
	pages={399--410},
	year={2016},
	organization={IEEE}
}

@inproceedings{ping2017datasynthesizer,
	title={Datasynthesizer: Privacy-preserving synthetic datasets},
	author={Ping, Haoyue and Stoyanovich, Julia and Howe, Bill},
	booktitle={Proceedings of the 29th International Conference on Scientific and Statistical Database Management},
	pages={1--5},
	year={2017}
}

@ARTICLE{image_augmentation,
	author={Ha, T.M. and Bunke, H.},
	journal={IEEE Transactions on Pattern Analysis and Machine Intelligence}, 
	title={Off-line, handwritten numeral recognition by perturbation method}, 
	year={1997},
	volume={19},
	number={5},
	pages={535-539},
	doi={10.1109/34.589216}}

@article{shorten2019survey,
	title={A survey on image data augmentation for deep learning},
	author={Shorten, Connor and Khoshgoftaar, Taghi M},
	journal={Journal of Big Data},
	volume={6},
	number={1},
	pages={1--48},
	year={2019},
	publisher={Springer}
}

@article{perez2017effectiveness,
	title={The effectiveness of data augmentation in image classification using deep learning},
	author={Perez, Luis and Wang, Jason},
	journal={arXiv preprint arXiv:1712.04621},
	year={2017}
}

@article{raschka2017python,
	title={Python Machine Learning: Machine Learning and Deep Learning with Python},
	author={Raschka, Sebastian and Mirjalili, Vahid},
	journal={Scikit-Learn, and TensorFlow. Second edition ed},
	year={2017}
}

@inproceedings{lighthill1973artificial,
	title={Artificial Intelligence: A General Survey},
	author={Lighthill, I},
	booktitle={Artificial Intelligence: A Paper Symposium. London: Science Research Council},
	year={1973}
}